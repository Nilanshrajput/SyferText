{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SyferText imports\n",
    "import syfertext\n",
    "from syfertext.pipeline import SimpleTagger\n",
    "\n",
    "# Import useful utility functions for this tutoria\n",
    "from utils import download_dataset\n",
    "\n",
    "# PySyft and PyTorch import\n",
    "import syft as sy\n",
    "from syft.generic.string import String\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "\n",
    "import os\n",
    "import codecs\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to download dataset: `conll2003` ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "conll2003: 4.86MB [00:00, 6.48MB/s]                            \n"
     ]
    }
   ],
   "source": [
    "# The URL template to all dataset files\n",
    "url_template = 'https://github.com/Nilanshrajput/ner_dataset/blob/master/conll2003/%s'\n",
    "\n",
    "# File names to be downloaded from the using the URL template above\n",
    "files = ['eng.train','eng.testa','eng.testb']\n",
    "\n",
    "# Construct the list of urls\n",
    "urls = [url_template % file for file in files]\n",
    "\n",
    "\n",
    "# The dataset name and its root folder\n",
    "dataset_name = 'conll2003'\n",
    "root_path = './conll2003'\n",
    "\n",
    "# Create the dataset folder if it is not already there\n",
    "if not os.path.exists('./conll2003'):\n",
    "    os.mkdir('./conll2003')\n",
    "\n",
    "# Start downloading\n",
    "download_dataset(dataset_name = dataset_name, \n",
    "                 urls = urls, \n",
    "                 root_path = root_path\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Work Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Torch was already hooked... skipping hooking process\n"
     ]
    }
   ],
   "source": [
    "# Create a torch hook for PySyft\n",
    "hook = sy.TorchHook(torch)\n",
    "\n",
    "# Create some PySyft workers\n",
    "me = hook.local_worker # This is the worker representing the deep learning company\n",
    "bob = sy.VirtualWorker(hook, id = 'bob') # Bob owns the first dataset\n",
    "alice = sy.VirtualWorker(hook, id = 'alice') # Alice owns the second dataset\n",
    "\n",
    "crypto_provider = sy.VirtualWorker(hook, id = 'crypto_provider') # provides encryption primitive for SMPC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_digits(s):\n",
    "    \"\"\"\n",
    "    Replace every digit in a string by a zero.\n",
    "    \"\"\"\n",
    "    return re.sub('\\d', '0', s)\n",
    "\n",
    "def load_sentences(path, zeros):\n",
    "    \"\"\"\n",
    "    Load sentences. A line must contain at least a word and its tag.\n",
    "    Sentences are separated by empty lines.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    for line in codecs.open(path, 'r', 'utf8'):\n",
    "        line = zero_digits(line.rstrip()) if zeros else line.rstrip()\n",
    "        if not line:\n",
    "            if len(sentence) > 0:\n",
    "                if 'DOCSTART' not in sentence[0][0]:\n",
    "                    sentences.append(sentence)\n",
    "                sentence = []\n",
    "        else:\n",
    "            word = line.split()\n",
    "            assert len(word) >= 2\n",
    "            sentence.append(word)\n",
    "    if len(sentence) > 0:\n",
    "        if 'DOCSTART' not in sentence[0][0]:\n",
    "            sentences.append(sentence)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = load_sentences('conll2003/eng.train',zeros=True)\n",
    "test_sentences = load_sentences('conll2003/eng.testb', zeros=True)\n",
    "dev_sentences = load_sentences('conll2003/eng.testa', zeros=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Peter', 'NNP', 'I-NP', 'I-PER'], ['Blackburn', 'NNP', 'I-NP', 'I-PER']]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']\n",
      "EU rejects German call to boycott British lamb .\n"
     ]
    }
   ],
   "source": [
    "for s in train_sentences:\n",
    "        \n",
    "        str_words = [w[0] for w in s]\n",
    "        print(str_words)\n",
    "        print(\" \".join(str_words))\n",
    "        break\n",
    "        #the last word in every sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def tag_mapping(sentences):\n",
    "    \"\"\"\n",
    "    Create a mapping of tags(item to ID / ID to item)  \n",
    "    \"\"\"\n",
    "    tags = set()\n",
    "    \n",
    "    for s in sentences:\n",
    "        tags = tags.union({word[-1] for word in s})\n",
    "    \n",
    "    tag_to_id, id_to_tag = {},{}\n",
    "    for id,tag in enumerate(tags):\n",
    "        tag_to_id[tag]=id\n",
    "        id_to_tag[id]=tag\n",
    "    print(\"Found %i unique named entity tags\" % len(tags))\n",
    "    return list(tags),tag_to_id, id_to_tag"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Create tag mapping\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 unique named entity tags\n"
     ]
    }
   ],
   "source": [
    "tags, tag_to_id, id_to_tag = tag_mapping(train_sentences+test_sentences+dev_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14041 / 3250 / 3453 sentences in train / dev / test.\n"
     ]
    }
   ],
   "source": [
    "def lower_case(x,lower=False):\n",
    "    if lower:\n",
    "        return x.lower()  \n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def prepare_dataset(sentences, tag_to_id, lower=False):\n",
    "    \"\"\"\n",
    "    Prepare the dataset. Return a list of lists of dictionaries containing: words and tag indexes\n",
    "    \"\"\"\n",
    "    \n",
    "    data = []\n",
    "    tag\n",
    "    for s in sentences:\n",
    "        \n",
    "        str_words = [w[0] for w in s]\n",
    "        #the last word in every sentence represetn ner tag\n",
    "        tags = [tag_to_id[w[-1]] for w in s]\n",
    "        data.append({\n",
    "            'string': \" \".join(str_words),\n",
    "            'tags': tags,\n",
    "        })\n",
    "        \n",
    "    return data\n",
    "\n",
    "train_data = prepare_dataset(\n",
    "    train_sentences, tag_to_id, lower=True)\n",
    "dev_data = prepare_dataset(\n",
    "    dev_sentences, tag_to_id, lower=True)\n",
    "test_data = prepare_dataset(\n",
    "    test_sentences, tag_to_id, lower=True)\n",
    "print(\"{} / {} / {} sentences in train / dev / test.\".format(len(train_data), len(dev_data), len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
